{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reload config whenever there are changes to avoid stupid cache issues\n",
    "importlib.reload(config)\n",
    "\n",
    "# Parameters from training\n",
    "img_width, img_height = config.img_width, config.img_height\n",
    "class_names = config.class_names\n",
    "\n",
    "TF_MODEL_FILE_PATH = config.TF_MODEL_FILE_PATH\n",
    "BATCH_SIZE = 12500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was foolish enough to try and run this multi threaded initially... until I discovered TF Lite is multithreaded itself so we can just adjust the BATCH param and make sure the architecture of the model allows it. It's best to build a tf lite model that serves batch processing straight away I found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected all file paths, total tiles: 1593368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 1593368/1593368 [2:34:34<00:00, 171.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and logging completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the interpreter\n",
    "def load_interpreter():\n",
    "    interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    return interpreter, input_details, output_details\n",
    "\n",
    "interpreter, input_details, output_details = load_interpreter()\n",
    "\n",
    "\n",
    "def open_imgs(img_paths):\n",
    "    batch_imgs = []\n",
    "    for img_path in img_paths:\n",
    "        img = tf.keras.utils.load_img(img_path, target_size=(img_height, img_width))\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        batch_imgs.append(img_array)\n",
    "    return np.array(batch_imgs)\n",
    "\n",
    "\n",
    "def predict_and_classify_images(batch_imgs):\n",
    "    batch_imgs = batch_imgs.astype(np.float32)  # Ensure the batch is of type float32\n",
    "    interpreter.resize_tensor_input(input_details[0]['index'], [len(batch_imgs), img_height, img_width, 3])\n",
    "    interpreter.allocate_tensors()  # Reallocate tensors after resizing\n",
    "    interpreter.set_tensor(input_details[0]['index'], batch_imgs)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "    results = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        score = tf.nn.softmax(predictions[i])\n",
    "        results.append((class_names[np.argmax(score)], 100 * np.max(score)))\n",
    "    return results\n",
    "\n",
    "# dirs\n",
    "input_directory = './data/02-source-map-tiles'\n",
    "log_file_path = config.log_file_path\n",
    "\n",
    "# gather all file paths\n",
    "all_files = [os.path.join(root, file) for root, _, files in os.walk(input_directory) for file in files if os.path.isfile(os.path.join(root, file))]\n",
    "total_files = len(all_files)\n",
    "print(f\"Collected all file paths, total tiles: {total_files}\")\n",
    "\n",
    "# Process and log images\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    with tqdm(total=total_files, desc=\"Processing images\") as pbar:\n",
    "        for i in range(0, total_files, BATCH_SIZE):\n",
    "            batch_paths = all_files[i:i + BATCH_SIZE]\n",
    "            batch_imgs = open_imgs(batch_paths)\n",
    "            predictions = predict_and_classify_images(batch_imgs)\n",
    "            for path, (pred_class, confidence) in zip(batch_paths, predictions):\n",
    "                log_file.write(f\"{path},{pred_class},{confidence:.2f}\\n\")\n",
    "            pbar.update(len(batch_paths))\n",
    "\n",
    "print(\"Processing and logging completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
